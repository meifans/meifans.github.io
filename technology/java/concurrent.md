#并发

## 并发包

### ThreadPoolExecutor
一个ExecutorService(接口)实现，使用几个线程执行每个提交的task，通常可以使用Executors的工厂方法来配置。

**线程池解决了2个问题**
  + 当执行大量的异步任务时，它提供更高的性能，因为它减少了每个任务执行前创建线程的消耗。
  + 提供了一种手段，准备和管理执行一个集合的任务所需要的资源，包括线程，消耗。同时也统计了一些数据，例如完成任务的个数等。

**为了便于使用，提供了Executors工厂**
  + newCachedThreadPool 线程数无限，自动扩容。
  + newFixedThreadPool 固定线程数量。
  + newSingleThreadExecutor 单个线程。

**提供了参数调整**
  + corePoolSize：        线程池维护线程的最少数量
  + maximumPoolSize：     线程池维护线程的最大数量
  + keepAliveTime：       线程池维护线程所允许的空闲时间
  + unit：                线程池维护线程所允许的空闲时间的单位
  + workQueue：           线程池所使用的缓冲队列
    - 运行线程数<core    首选增加一个线程而不是放到队列
    - 运行线程数>=core   首选把新任务放到队里，而不是创建一个线程
    - 如果无法放到队列，新建一个线程处理任务，除非已经超过了最大线程数。此时任务被拒绝。
  + handler：             线程池对拒绝任务的处理策略

**有三种Queue的选择**
  + SynchronousQueue ： 直接提交，而不是放到queue。通常需要无限的maximumPoolSize,CachedThreadPool使用。
    + 无界队列，但是由于本身的特性，`某次添加元素后必须等待其他线程取走后才能继续添加`。
    + 可以避免在处理具有内部依赖性的请求时出现锁。因为b依赖于a,先后提交a,b,可以保证a必定先被执行。

  + LinkedBlockingQueue ： 无限队列,当所有线程繁忙时，会把任务放到queue中等待。FixedThreadPool和SingleThreadExecutor 使用。
    > corePoolSize熟练的线程一直在跑，永远也不会触发新的线程 （因为队列无界）

  + ArrayBlockingQueue : 固定数量队列。当maximumPoolSize数量固定时，防止资源枯竭。

## 修饰符

### volatile

#### 术语定义
| 术语 | 英文单词 | 描述 |
| --- | ------  | --- |
| 共享变量 |    |   在多个线程之间能够被共享的变量被称为共享变量。java中共享变量包括所有的实例变量，静态变量和数组元素。他们都被存放在堆内存中，Volatile只作用于共享变量。|
| 内存屏障| Memory Barriers | 是一组处理器指令，用于实现对内存操作的顺序限制。 |
| 缓冲行 | Cache line  | 缓存中可以分配的最小储存单位。处理器填写缓存行时会加载整个缓存行，需要使用多个朱内存读周期 |
| 原子操作 | Atomic operations | 不可中断的一个或一系列操作。 |
| 缓存行填充 | cache line fill | 当处理器识别到从内存中读取操作数是可缓存的，处理器读取整个缓存行到适当的缓存（L1，L2，L3的或所有） |
| 缓存命中 | cache hit | 如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是从内存。 |
| 写命中 | write hit | 当处理器将操作数写回到一个内存缓存的区域时，它首先会检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中。 |
| 写缺失 | write misses the cache | 一个有效的缓存行被写入到不存在的内存区域。 |

#### volatile 作用
  1. 保证内存可见性
  2. 防止指令重排

**内存可见性**

  概念：JVM 内存模型

  ![JVM内存模型](pictures/JVM内存模型.png)

  Java内存模型规定，`共享变量`储存在主内存中，每个线程都有自己的工作内存（比如CPU寄存器），线程只能访问自己的工作内存，不能访问其他线程的工作内存。

  每个线程的工作内存中都保存了主内存中共享变量的副本，线程要要操作共享变量，只能通过操作自己工作内存中的共享变量副本来实现，操作结束后在同步会主内存。

  为了保证多线程操作主内存数据完整性，Java内存模型规定工作内存与主内存之间交互的协议，定义了8种原子操作：
    1. lock:将主内存中的变量锁定，为一个线程所独占

    2. unclock:将lock加的锁定解除，此时其它的线程可以有机会访问此变量

    3. read:将主内存中的变量值读到工作内存当中

    4. load:将read读取的值保存到工作内存中的变量副本中

    5. use:将值传递给线程的代码执行引擎

    6. assign:将执行引擎处理返回的值重新赋值给变量副本

    7. store:将变量副本的值存储到主内存中

    8. write:将store存储的值写入到主内存的共享变量当中

  >可见性的一个例子
  >
  > 上面线程A和B都访问一个数据，假设是一个布尔变量，初始值为true。当线程A获得锁（lock）之后在自己的工作内存操作共享变量,操作完之后将自己工作内存中的false副本写入主内存。此时主内存是false.
  >
  > B一直在做一个循环根据共享变量，那么在B从共享内存将false同步回自己的工作内存之前，A对于布尔变量的改变，B是不可见的（即A操作之前的值）。
  > 注意,最终是一定会同步会主内存中，但是时间不是立刻。
  >
  >此时，将布尔变量标志为 volatile，就能保证B立刻终止。

  `volatile保证可见性的原理`

 在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。所以volatile保证变量修改的实时可见性。


**缓存**

CPU直接访问主内存是非常慢的，为了提高速度，cpu与主内存之间有几层缓存。
>如果你正对一块数据做相同的运算，执行运算的时候把它加载到离cpu更近的地方就很有必要了。

![缓存结构](pictures/cache.png)

越靠近cpu，缓存越快，越小。L1（一级缓存）很小但非常快。L2更大些也更慢些，并且依旧只能被一个单独的CPU核使用。
L3更多的在多核处理器上，更大更慢，但它是被`单个插槽上所有CPU共享`。最后所有的CPU共享主存。
>当CPU执行运算的时候，它先去L1查找所需的数据，再去L2，然后是L3，最后如果这些缓存中都没有，所需的数据就要去主内存拿。走得越远，运算耗费的时间就越长。

Martin和Mike的 QCon presentation演讲中给出了一些缓存未命中的消耗数据：

| 从CPU到	| 大约需要的 CPU 周期	| 大约需要的时间 |
| ---- | --------------------| ------------ |
| 主存	| 	| 约60-80纳秒 |
| QPI总线传输(between sockets, not drawn) | 	|	约20ns |
| L3 cache | 约40-45 cycles | 	约15ns |
| L2 cache	| 约10 cycles |	约3ns |
| L1 cache	| 约3-4 cycles |	约1ns |
| 寄存器	| 1 cycle |   |
